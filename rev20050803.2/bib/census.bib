This file was created with JabRef 1.6 beta.
Encoding: ISO8859_1

@BOOK{Vality2000,
  title = {The {INTEGRITY} Data Re-engineering Environment, {SuperMATCH} Concepts and Reference},
  publisher = {Vality Technology Inc.},
  year = {2000},
  author = {Anonymous},
  edition = {Version 3.0},
  month = {October},
  note = {Vality Technology Inc was bought by Ascential Software, Inc. in 2002.},
}

@ARTICLE{FellegiSunter1969,
  author = {Fellegi, I. P. and A. B. Sunter},
  title = {A Theory for Record Linkage},
  journal = jasa,
  year = {1969},
  volume = {64},
  pages = {1183-1210},
}

@BOOK{Jaro1997,
  title = {AUTOMATCH {G}eneralized {R}ecord {L}inkage System},
  publisher = {MatchWare Technologies, Inc.},
  year = {1997},
  author = {Jaro, Mathew A.},
  address = {Burtonsville, Maryland, 20866},
  edition = {Version 4.2},
  note = {Matchware Technologies Inc. was bought by Vality Technology Inc, and is now owned by Ascential Software, Inc.},
}

@ARTICLE{Jaro1989,
  author = {Jaro, Mathew A.},
  title = {Advances in Record-Linkage Methodology as Applied to Matching the 1985 {C}ensus of {T}ampa, {F}lorida},
  journal = jasa,
  year = {1989},
  volume = {89},
  pages = {414-420},
  abstract = {referenced in Winkler1993},
}

@ARTICLE{NewcombeEtAl1959,
  author = {Newcombe, H. B. and J. M. Kennedy and S. J. Axford and A. P. James},
  title = {Automatic Linkage of Vital Records},
  journal = {Science},
  year = {1959},
  volume = {130},
  pages = {954-959},
}

@ARTICLE{ScheurenWinkler1997,
  author = {Scheuren, Fritz, and Winkler, William E.},
  title = {Regression analysis of data files that are computer matched - Part {II}},
  journal = {Survey Methodology},
  year = {1997},
  volume = {23},
  pages = {157-165},
  owner = {vilhuber},
  keywords = {regression tracking matching},
}

@ARTICLE{ScheurenWinkler1993,
  author = {Scheuren, Fritz, and Winkler, William E.},
  title = {Regression analysis of data files that are computer matched},
  journal = {Survey Methodology},
  year = {1993},
  volume = {19},
  pages = {39-58},
  owner = {vilhuber},
  keywords = {regression tracking matching},
}

@TECHREPORT{ThibaudeauWinkler2002,
  author = {Yves Thibaudeau and William E. Winkler},
  title = {Bayesian Networks Representations, Generalized Imputation, and Synthetic Micro-data satisfying Analytic Constraints},
  institution = {U.S. Census},
  year = {2002},
  type = {SRD Research Report},
  number = {RRS2002/09},
  abstract = {This paper shows how Bayesian Networks can be used to create models for
	 discrete data from contingency tables. The advantage is that the models
	 are created relatively automatically using existing software. The
	 models provide representations that approximately preserve the joint
	 relationships of variables and are easy to apply. The models allow
	 imputation for missing data in contingency tables and for the creation
	 of discrete, synthetic microdata satisfying analytic constraints.},
  owner = {vilhuber},
  pdf = {T/ThibaudeauWinkler2002.pdf},
}

@TECHREPORT{Winkler2001-03,
  author = {William E. Winkler},
  title = {Record Linkage Software and Methods for Merging Administrative Lists},
  institution = {U.S. Census Bureau},
  year = {2001},
  type = {Research Report Series},
  number = {RR2001/03},
  owner = {vilhuber},
  abstract = {National Statistical Institutes often have the
	 need to merge administrative files from a variety of sources for
	 which unique identifiers are not available to facilitate matching.
	 Agencies such as Eurostat have the need to connect data sources from
	 different countries and sources and to verify the confidentiality
	 of microdata. To do this merging of administrative lists, agencies
	 need fast software for cleaning up and standardizing lists and for
	 merging the lists. The U.S. Bureau of the Census has software for name
	 standardization, address standardization, and matching that are considered
	 state-of-the-art. The standardization software breaks names and addresses into
	 components that are easily compared. The matching software accounts for
	 typographical error, automatically estimates matching parameters, and
	 optimizes sets of assignments over large groups of pairs of records.},
  keywords = {matching, standardization},
  pdf = {W/Winkler2001-03.pdf},
}

@TECHREPORT{Winkler2000-05,
  author = {William E. Winkler},
  title = {Using the {EM} algorithm for weight computation in the {F}ellegi-{S}unter model of record linkage},
  institution = {U.S. Census Bureau},
  year = {2000},
  type = {Research Report Series},
  number = {RR00/05},
  abstract = {Let A×B be the product space of two
	 sets A and B which is divided into a (pairs representing the same
	 entity) and nonmatches (pairs representing different entities). Linkage
	 rules are those that divide A×B into links (designated matches),
	 possible links (pairs for which we delay a decision), and nonlinks
	 (designated nonmatches). Under fixed bounds on the error rates, Fellegi
	 and Sunter (1969) provided a linkage rule that is optimal in the
	 sense that it minimizes the set of possible links. The optimality
	 is dependent on knowledge of certain joint inclusion probabilities
	 that are used in a crucial likelihood ratio. In applying the record
	 linkage model, assumptions are often made that allow estimation of
	 weights that are a function of the joint inclusion probabilities.
	 If the assumptions are not met, then the linkage procedure using
	 estimates computed under the assumptions may not be optimal. This paper
	 describes a method for estimating weights using the EM Algorithm under
	 less restrictive assumptions. The weight computation automatically
	 incorporates a Bayesian adjustment based on file characteristics.},
  owner = {vilhuber},
  keywords = {decision rule, error rate},
  pdf = {W/Winkler2000-05.pdf},
}

@TECHREPORT{Winkler2000-06,
  author = {William E. Winkler},
  title = {Frequency-Based Matching in {F}ellegi-{S}unter Model of Record Linkage},
  institution = {U.S. Census Bureau},
  year = {2000},
  type = {Research Report Series},
  number = {RR00/06},
  abstract = {This paper extends techniques for
	 frequency-based matching (see e.g., Fellegi and Sunter 1969). The
	 extended techniques allow table-building under weaker assumptions
	 than those typically used in practice. Although CPU requirements
	 can increase, human intervention can be reduced in some situations.},
  owner = {vilhuber},
  keywords = {decision rule, error rate},
  pdf = {W/Winkler2000-06.pdf},
}

@TECHREPORT{Winkler1999-01,
  author = {William E. Winkler},
  title = {State of statistical data editing and current research problems},
  institution = {U. S. Bureau of the Census},
  year = {1999},
  type = {Research Report Series },
  number = {99/01},
  address = {Washington, D.C.},
  abstract = {This paper provides background
	 on Fellegi-Holt methods as they are implemented for discrete and
	 continuous data. It describes several Fellegi-Holt systems that are
	 currently in use throughout the world in different statistical agencies.
	 Because all the systems have limitations, it delineates a few research
	 problems that, if solved, would improve the use of the systems. },
}

@TECHREPORT{Winkler1999-04,
  author = {William E. Winkler},
  title = {The State of Record Linkage and Current Research Problems},
  institution = {U. S. Bureau of the Census},
  year = {1999},
  type = {Research Report Series },
  number = {99/04},
  address = {Washington, D.C.},
  abstract = {This paper provides an overview
	 of methods and systems developed for record linkage. Modern record
	 linkage begins with the pioneering work of Newcombe and is especially
	 based on the formal mathematical model of Fellegi and Sunter. In
	 their seminal work, Fellegi and Sunter introduced many powerful ideas
	 for estimating record linkage parameters and other ideas that still
	 influence record linkage today. Record linkage research is characterized
	 by its synergism of statistics, computer science, and operations
	 research. Many difficult algorithms have been developed and put in
	 software systems. Record linkage practice is still very limited.
	 Some limits are due to existing software. Other limits are due to
	 the difficulty in automatically estimating matching parameters and
	 error rates, with current research highlighted by the work of Larsen
	 and Rubin. Still other limits are due to the inability do auxiliary
	 programming to clean-up and adjust for data-specific anomalies. The
	 evaluation of matching results also necessitates auxiliary analyses.
	 The paper closes with a description of selected research problems.},
}

@TECHREPORT{Winkler1993,
  author = {William E. Winkler},
  title = {Matching and Record Linkage},
  institution = {U. S. Bureau of the Census},
  year = {1993},
  type = {Research Report Series },
  number = {93/08},
  address = {Washington, D.C.},
  abstract = {Record linkage is used in creating a frame,
	 removing duplicates from files, or combining files so that relationships
	 on two or more data elements from separate files can be studied.
	 Much of the record linkage work in the past has been manual or via
	 elementary but ad hoc rules. The report provides an overview of matching
	 techniques that are based on formal mathematical models subject to testing
	 via statistical and other accepted methods. The examples relate to
	 business applications. Extensive references are provided. This report
	 will appear in the book, "Survey Methods for Businesses, Farms, and
	 Institutions" that will be published by Wiley- Interscience in 1994.},
}

@TECHREPORT{Winkler1991-09,
  author = {William E. Winkler and Yves Thibaudeau},
  title = {An application of the {F}ellegi-{S}unter model of record linkage to the 1990 {U.S.} {D}ecennial {C}ensus},
  institution = {U. S. Bureau of the Census},
  year = {1991},
  type = {Research Report Series },
  number = {91/9},
  address = {Washington, D.C.},
  abstract = { },
}

@TECHREPORT{Yancey2004-01,
  author = {William E. Yancey},
  title = {Improving EM Algorithm Estimates for Record Linkage Parameters},
  institution = {U.S. Census Bureau},
  year = {2004},
  type = {Research Report Series},
  number = {RRS2004/01},
  owner = {vilhuber},
  abstract = {The EM algorithm can be used to estimate conditional
	 probabilities for matching field patterns for the Fellegi-Sunter model
	 for record linkage. The algorithm is based on a latent class model
	 for the record pairs where one of the classes is the set of true
	 matches. If the number of true match pairs in the data set is too small,
	 then the EM algorithm cannot detect the correct latent class. We
	 consider methods for enriching the density of matches in the set
	 of examined record pairs in order to obtain improved EM algorithm
	 estimates for the record linkage conditional probability parameters.},
  pdf = {Y/Yancey2004-01.pdf},
}

@TECHREPORT{Yancey2000-07,
  author = {William E. Yancey},
  title = {Frequency-Dependent Probability Measures for Record Linkage},
  institution = {U.S. Census Bureau},
  year = {2000},
  type = {Research Report Series},
  number = {RR00/07},
  owner = {vilhuber},
  abstract = {Record linkage procedures based on the Fellegi-Sunter Theory (JASA, 1969) require the
	 estimation of the conditional probabilities of the agreement patterns.
	 Under the assumption of conditional independence, this reduces to the
	 estimation of the conditional probabilities of the agreement of the
	 individual matching fields. We consider methods for using value-specific,
	 frequency-based methods to modify the agreement probabilities according to
	 the rate of recurrence of the common matching field value in the
	 matching set. We compare and analyze the effects of the methods when
	 applied to Census data sets, and assess their value and usability.},
  pdf = {Y/Yancey2000-07.pdf},
}

